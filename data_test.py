message = """Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Lorem ipsum dolor sit amet,
consectetur adipiscing elit,
sed do eiusmod tempor incididunt
ut labore et dolore magna aliqua.
sed do eiusmod tempor incididunt
tempor??
Here is what's happening internally: 
index is going through all values starting 
from the 1st position (0th index) looking for 
the element you are searching for, and  as soon 
as it finds the value it returns the position 
and exits the system. However, this is not too 
efficient when going through a large list and 
you need to get the position of something 
towards the end of the however list.
Thiết kế cơ sở dữ liệu cho quản 
lý giáo viên của trường học viện 
kỹ thuật quân sự. Mô tả mô hình 
quan hệ quản lý giáo viên, 
các thủ tục (procedure) và các 
hàm (function) truy xuất dữ liệu 
trong SQLServer, phần giao diện 
viết bằng c# winform.
Sáu học sinh Việt Nam tham gia Olympic Tin học châu Á Thái Bình Dương đều giành huy chương, 
trong đó có một vàng, bốn bạc và một đồng.
Ngày 22/8, Cục Quản lý chất lượng (Bộ Giáo dục và Đào tạo) thông tin em Bùi Hồng Đức, 
học sinh lớp 12 trường THPT chuyên Khoa học Tự nhiên (Đại học Khoa học tự nhiên, Đại học Quốc gia Hà Nội) giành huy chương vàng. 
Năm ngoái, khi là học sinh lớp 11, Đức đoạt huy chương vàng Olympic quốc tế.
Bốn em giành huy chương bạc là Vũ Hoàng Kiên (lớp 12), Lê Quang Huy (lớp 11), 
Nguyễn Đình Phúc (lớp 11) đến từ trường chuyên Khoa học Tự nhiên và em Trần Quang Thành (lớp 12 trường THPT chuyên Đại học Sư phạm Hà Nội).
Em Đào Quang Thái Dương, học sinh lớp 12 trường THPT chuyên Trần Phú (Hải Phòng), giành huy chương đồng.
Olympic Tin học châu Á Thái Bình Dương năm 2020 được tổ chức bởi Indonesia, 
theo hình thức thi trực tuyến với sự tham gia của 33 quốc gia và vùng lãnh thổ. 
Đội tuyển Việt Nam thi trực tuyến tại Đại học Công nghệ, Đại học Quốc gia Hà Nội hôm 15/8.
Tại kỳ thi, có 241 thí sinh được tham gia xét giải. Kết quả, 120 em đoạt giải, chiếm 49,8%. 
Ngoài Việt Nam, chín nước và vùng lãnh thổ khác có huy chương vàng gồm: Trung Quốc (10), 
Nga (4), Iran (2), Hàn Quốc, Nhật Bản, Indonesia, Israel, Hong Kong và Georgia mỗi nơi một.
Theo xếp hạng huy chương, Việt Nam đứng thứ sáu sau Trung Quốc, Nga, Iran, Hàn Quốc và Nhật Bản. 
Thành tích này tốt hơn năm trước, khi Việt Nam được 7 huy chương bạc ở đấu trường châu Á.
Cục Quản lý chất lượng đánh giá kết quả Olympic Tin học châu Á Thái Bình Dương của Việt Nam đã khẳng định nỗ lực, 
chủ động ôn tập, bồi dưỡng của học sinh, giáo viên và nhà trường trong bối cảnh Covid-19 diễn biến phức phức tạp.
Another popular type of feed-forward network is the radial basis function (RBF)
network. It has two layers, not counting the input layer, and differs from a multilayer
perceptron in the way that the hidden units perform computations. Each hidden unit
essentially represents a particular point in input space, and its output, or activation,
for a given instance depends on the distance between its point and the instance,
which is just another point. Intuitively, the closer these two points, the stronger the
activation. This is achieved by using a nonlinear transformation function to convert
the distance into a similarity measure. A bell-shaped Gaussian activation function,
of which the width may be different for each hidden unit, is commonly used for this
purpose. The hidden units are called RBFs because the points in instance space for
which a given hidden unit produces the same activation form a hypersphere or
hyperellipsoid. (In a multilayer perceptron, this is a hyperplane.)
The output layer of an RBF network is the same as that of a multilayer perceptron:
It takes a linear combination of the outputs of the hidden units and—in classification
problems—pipes it through the sigmoid function (or something with a similar shape).
The parameters that such a network learns are (a) the centers and widths of the
RBFs and (b) the weights used to form the linear combination of the outputs obtained
from the hidden layer. A significant advantage over multilayer perceptrons is that
the first set of parameters can be determined independently of the second set and
still produce accurate classifiers.
One way to determine the first set of parameters is to use clustering. The simple
k-means clustering algorithm described in Section 4.8 can be applied, clustering
each class independently to obtain k-basis functions for each class. Intuitively, the
resulting RBFs represent prototype instances. The second set of parameters is then
learned by keeping the first parameters fixed. This involves learning a simple linear
classifier using one of the techniques we have discussed (e.g., linear or logistic
regression). If there are far fewer hidden units than training instances, this can be
done very quickly.
A disadvantage of RBF networks is that they give every attribute the same weight
because all are treated equally in the distance computation, unless attribute weight
parameters are included in the overall optimization process. Thus, they cannot deal
effectively with irrelevant attributes, in contrast to multilayer perceptrons. Support
vector machines share the same problem. In fact, support vector machines with
Gaussian kernels (i.e., “RBF kernels”) are a particular type of RBF network, in
which one basis function is centered on every training instance, all basis functions
Another popular type of feed-forward network is the radial basis function (RBF)
network. It has two layers, not counting the input layer, and differs from a multilayer
perceptron in the way that the hidden units perform computations. Each hidden unit
essentially represents a particular point in input space, and its output, or activation,
for a given instance depends on the distance between its point and the instance,
which is just another point. Intuitively, the closer these two points, the stronger the
activation. This is achieved by using a nonlinear transformation function to convert
the distance into a similarity measure. A bell-shaped Gaussian activation function,
of which the width may be different for each hidden unit, is commonly used for this
purpose. The hidden units are called RBFs because the points in instance space for
which a given hidden unit produces the same activation form a hypersphere or
hyperellipsoid. (In a multilayer perceptron, this is a hyperplane.)
The output layer of an RBF network is the same as that of a multilayer perceptron:
It takes a linear combination of the outputs of the hidden units and—in classification
problems—pipes it through the sigmoid function (or something with a similar shape).
The parameters that such a network learns are (a) the centers and widths of the
RBFs and (b) the weights used to form the linear combination of the outputs obtained
from the hidden layer. A significant advantage over multilayer perceptrons is that
the first set of parameters can be determined independently of the second set and
still produce accurate classifiers.
One way to determine the first set of parameters is to use clustering. The simple
k-means clustering algorithm described in Section 4.8 can be applied, clustering
each class independently to obtain k-basis functions for each class. Intuitively, the
resulting RBFs represent prototype instances. The second set of parameters is then
learned by keeping the first parameters fixed. This involves learning a simple linear
classifier using one of the techniques we have discussed (e.g., linear or logistic
regression). If there are far fewer hidden units than training instances, this can be
done very quickly.
A disadvantage of RBF networks is that they give every attribute the same weight
because all are treated equally in the distance computation, unless attribute weight
parameters are included in the overall optimization process. Thus, they cannot deal
effectively with irrelevant attributes, in contrast to multilayer perceptrons. Support
vector machines share the same problem. In fact, support vector machines with
Gaussian kernels (i.e., “RBF kernels”) are a particular type of RBF network, in
which one basis function is centered on every training instance, all basis functions
and other kernel-based methods, including the optimization theory underlying the
support vector learning algorithms. We have barely skimmed the surface of these
learning schemes, mainly because advanced mathematics lies just beneath. The idea
of using kernels to solve nonlinear problems has been applied to many algorithms,
for example, principal components analysis (described in Section 7.3). A kernel is
essentially a similarity function with certain mathematical properties, and it is 
possible to define kernel functions over all sorts of structures—for example, sets, strings,
trees, and probability distributions. Shawe-Taylor and Cristianini (2004) and
Schölkopf and Smola (2002) cover kernel-based learning in detail.
There is extensive literature on neural networks, and Bishop (1995) provides
an excellent introduction to both multilayer perceptrons and RBF networks. Interest
in neural networks appears to have declined since the arrival of support vector
machines, perhaps because the latter generally require fewer parameters to be
tuned to achieve the same (or greater) accuracy. However, multilayer perceptrons
have the advantage that they can learn to ignore irrelevant attributes, and RBF
networks trained using k-means can be viewed as a quick-and-dirty method for
finding a nonlinear classifier. Recent studies have shown that multilayer perceptrons
achieve performance competitive with more modern learning techniques on many
practical datasets.
Recently there has been renewed interest in gradient methods for learning classifiers. 
In particular, stochastic gradient methods have been explored because they
are applicable to large data sets and online learning scenarios. Kivinen et al. (2002),
Zhang (2004), and Shalev-Shwartz et al. (2007) explore such methods when applied
to learning support vector machines. Kivinen et al. and Shalev-Shwartz et al. provide
heuristics for setting the learning rate for gradient descent based on the current iteration, 
which only require the user to provide a value for a single parameter that
determines the closeness of fit to the training data (a so-called regularization parameter). 
In the vanilla approach, regularization is performed by limiting the number of
"""